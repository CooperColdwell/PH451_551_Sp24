{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDkH5SC/f7YOF+TDOyOLq2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp24/blob/main/Resources/Pytorch_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak0NogfBAhrG"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sklearn.preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = 3000\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
      ],
      "metadata": {
        "id": "g6ectiV7AloN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X, y)\n",
        "plt.title('X and y before scaling')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LEVDHgb1BftG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "xscaler = scaler.fit(X)\n",
        "yscaler = scaler.fit(y)\n",
        "X = scaler.transform(X)\n",
        "y = scaler.transform(y)"
      ],
      "metadata": {
        "id": "x16rMKeklrkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X, y, color='orange')\n",
        "plt.title('X and y after scaling')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o0bzYgEwm8Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "OHhQsK7ylbXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PytorchRegressionDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.copy()).float()\n",
        "        self.y = torch.from_numpy(y.copy()).float()\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "VdORSNCclaUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = PytorchRegressionDataset(X_train, y_train)\n",
        "val_data = PytorchRegressionDataset(X_val, y_val)\n",
        "test_data = PytorchRegressionDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=5, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=5, shuffle=False)"
      ],
      "metadata": {
        "id": "q9Mym1dJlfq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD Algorithm for Linear Regression:\n",
        "\n",
        "$y_{pred}=\\Theta_0 + \\Theta_1x + ... + \\Theta_nx^n$ <br>\n",
        "$Loss = \\Sigma_i|y_{true,i}-y_{pred,i}|^2$ <br>\n",
        "$\\Theta_{i,new} = \\Theta_{i,old} - ( \\partial Loss /\\partial\\Theta_i )* \\eta$ $,\\eta =$ learning rate"
      ],
      "metadata": {
        "id": "iyVbzJNhICjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressor(nn.Module):\n",
        "    def __init__(self, degree):\n",
        "        super().__init__()\n",
        "        self.degree = degree\n",
        "        self.module = torch.nn.ModuleList()\n",
        "        for i in range(degree + 1):\n",
        "            self.module.append(nn.Linear(1, 1, bias=False))\n",
        "    def forward(self, x):\n",
        "        pred = torch.zeros(x.shape[0], 1)\n",
        "        for i in range(self.degree + 1):\n",
        "            pred = pred + self.module[i](x**i)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "oRkTTC_QB-Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegressor(degree=2)"
      ],
      "metadata": {
        "id": "3B-5n7LgJgw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(train_loader, val_loader, model, optimizer, criterion, metric, num_epochs):\n",
        "    history = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_metric': [],\n",
        "        'val_loss': [],\n",
        "        'val_metric': []\n",
        "    }  # Initialize a dictionary to store epoch-wise results\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_loss = 0.0  # Initialize the epoch loss and metric values\n",
        "        epoch_metric = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for X, y in train_loader:\n",
        "            optimizer.zero_grad()  # Clear existing gradients\n",
        "            outputs = model(X)  # Make predictions\n",
        "            loss = criterion(outputs, y)  # Compute the loss\n",
        "            loss.backward()  # Compute gradients\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_metric += metric(outputs, y)\n",
        "\n",
        "        # Average training loss and metric\n",
        "        epoch_loss /= len(train_loader)\n",
        "        epoch_metric /= len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            val_loss = 0.0\n",
        "            val_metric = 0.0\n",
        "            for X_val, y_val in val_loader:\n",
        "                outputs_val = model(X_val)  # Make predictions\n",
        "                val_loss += criterion(outputs_val, y_val).item()  # Compute loss\n",
        "                val_metric += metric(outputs_val, y_val)\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_metric /= len(val_loader)\n",
        "\n",
        "        # Append epoch results to history\n",
        "        history['epoch'].append(epoch_loss)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_metric'].append(epoch_metric)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_metric'].append(val_metric)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Train Metric: {epoch_metric:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'Val Metric: {val_metric:.4f}')\n",
        "\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "BXDbg78gneeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "metric = torch.nn.L1Loss()"
      ],
      "metadata": {
        "id": "GZdhi0jtnlmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = train_and_validate(train_loader, val_loader, model,\n",
        "                                    optimizer=optimizer, criterion=criterion,\n",
        "                                    metric=metric, num_epochs=50)"
      ],
      "metadata": {
        "id": "kibK-G9dn22U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(test_loader.dataset.X)"
      ],
      "metadata": {
        "id": "TgrfBp0nEXda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_test, y_test, label='Test Data')\n",
        "plt.scatter(X_test, predictions.detach().numpy(), label='Test Predictions deg=2')\n",
        "plt.legend()\n",
        "plt.title('X_test and y_test normal scaled')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JJX-0v9IEaRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(xscaler.inverse_transform(X_test), yscaler.inverse_transform(y_test), label='Test Data')\n",
        "plt.scatter(xscaler.inverse_transform(X_test), yscaler.inverse_transform(predictions.detach().numpy()), label='Test Predictions deg=2')\n",
        "plt.legend()\n",
        "plt.title('X_test and y_test original scale')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A29AOqA1pq2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegressor(degree=8)"
      ],
      "metadata": {
        "id": "NELrS6Y2Evh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "metric = torch.nn.L1Loss()"
      ],
      "metadata": {
        "id": "dbTImoDBqIfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = train_and_validate(train_loader, val_loader, model,\n",
        "                                    optimizer=optimizer, criterion=criterion,\n",
        "                                    metric=metric, num_epochs=50)"
      ],
      "metadata": {
        "id": "qskZkXF7qIfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(test_loader.dataset.X)"
      ],
      "metadata": {
        "id": "OB-OLfthqIfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_test, y_test, label='Test Data')\n",
        "plt.scatter(X_test, predictions.detach().numpy(), label='Test Predictions deg=8')\n",
        "plt.legend()\n",
        "plt.title('X_test and y_test normal scaled')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_iSswko_qIfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(xscaler.inverse_transform(X_test), yscaler.inverse_transform(y_test), label='Data')\n",
        "plt.scatter(xscaler.inverse_transform(X_test), yscaler.inverse_transform(predictions.detach().numpy()), label='Test Predictions deg=8')\n",
        "plt.legend()\n",
        "plt.title('X_test and y_test original scale')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dqPs0283qIfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's now compare SGD for linear regression to SGD for Neural Nets:\n",
        "\n",
        "### SGD Algorithm for Linear Regression:\n",
        "\n",
        "$y_{pred}=\\Theta_0 + \\Theta_1x_1 + ... + \\Theta_nx_n$ <br>\n",
        "$Loss = \\Sigma_i|y_{true,i}-y_{pred,i}|^2$ <br>\n",
        "$\\Theta_{i,new} = \\Theta_{i,old} - ( \\partial Loss /\\partial\\Theta_i )* \\eta$ $,\\eta =$ learning rate"
      ],
      "metadata": {
        "id": "ednZo1gxOtnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD Optimizer for Linear Models with Mean Square Error Loss Function:\n",
        "\n",
        "Consider just the neuron activation for the first neuron in the last layer. <br>\n",
        "This neuron is essentially what our model prediction is for the first output $y_0$.\n",
        "\n",
        "$a_{0,l} = \\sigma[w_{0,l}(a_{0,l-1} + a_{1,l-1} + ... a_{m,l-1})+ b_l]$ <br>\n",
        "a are activations of neurons, b are layer biases, w are neuron weights, $\\sigma$ <br>\n",
        "is some sort of activation function <br>\n",
        "\n",
        "Consider now how we can evaluate this loss using MSE: <br>\n",
        "$Loss_{0,l} = |y_{true,0}-a_{0,l}|^2$ <br>\n",
        "\n",
        "We can update our weights just like we updated our $\\Theta$ values in linear reg. <br>\n",
        "$w_{0,l,new} = w_{0,l,old} - ( \\partial Loss /\\partial w_{0,l} )* \\eta$ $,\\eta =$ learning rate <br>\n",
        "\n",
        "Same for biases. <br>\n",
        "$b_{l,new} = b_{l,old} - ( \\partial Loss /\\partial b_l )* \\eta$\n",
        "\n",
        "We then repeat this procedure going from the end of the model backward to the <br>\n",
        "start of the model updating all of the neuron weights and layer biases. Note <br>\n",
        "that the neurons in the second-to-last layer would need to have their losses <br>\n",
        "partial derivatives summed over all activations in the final layer since <br> they're connected to all of the final layer neurons.\n",
        "\n",
        "It turns out that this process is totally general and can be used for many loss <br>\n",
        "functions and activation functions. Most other optimizer algorithms are just <br>\n",
        "improvements aimed at smoothing the behavior of SGD."
      ],
      "metadata": {
        "id": "h_STU8RpJugX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential classifier for 10 1D inputs 30 hidden dimensions and 1 binary class\n",
        "sequential_model1 = nn.Sequential()\n",
        "sequential_model1.append(nn.Linear(10, 30))\n",
        "sequential_model1.append(nn.ReLU())\n",
        "sequential_model1.append(nn.Linear(30, 1))\n",
        "sequential_model1.append(nn.Sigmoid())\n",
        "\n",
        "optim = torch.optim.SGD(sequential_model1.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "CXKIo895FMYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential classifier for 10x10 2D inputs 50 hidden dimensions and 1 binary class\n",
        "sequential_model2 = nn.Sequential()\n",
        "sequential_model2.append(nn.Flatten())\n",
        "sequential_model2.append(nn.Linear(10*10, 50))\n",
        "sequential_model2.append(nn.ReLU())\n",
        "sequential_model2.append(nn.Linear(50, 1))\n",
        "sequential_model2.append(nn.Sigmoid())\n",
        "\n",
        "optim = torch.optim.SGD(sequential_model2.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "hkOlooWGGlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential classifier for 20x30 1D inputs 30 hidden dimensions and 10 unique classes\n",
        "sequential_model3 = nn.Sequential()\n",
        "sequential_model3.append(nn.Flatten())\n",
        "sequential_model3.append(nn.Linear(20*30, 30))\n",
        "sequential_model3.append(nn.ReLU())\n",
        "sequential_model3.append(nn.Linear(30, 10))\n",
        "sequential_model3.append(nn.Softmax())\n",
        "\n",
        "optim = torch.optim.SGD(sequential_model3.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "FnRxZEQuIP3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential regressor for 20 1D inputs 50 hidden dimensions and 3 outputs\n",
        "sequential_model4 = nn.Sequential()\n",
        "sequential_model4.append(nn.Linear(20, 50))\n",
        "sequential_model4.append(nn.ReLU())\n",
        "sequential_model4.append(nn.Linear(50, 3))\n",
        "\n",
        "optim = torch.optim.SGD(sequential_model4.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "OlIgSTK_G2d7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}